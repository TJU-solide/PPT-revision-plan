\documentclass[12pt,one-column]{article}
\usepackage{url}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{wrapfig}
\usepackage{psfrag}
\usepackage{comment}
\usepackage{color}
\usepackage[tight,footnotesize]{subfigure}
\usepackage{titlesec}
\usepackage{setspace}
\usepackage{makecell}
\usepackage{indentfirst}
\usepackage{bm}
\usepackage{enumitem}


\input{new-commands}
\oddsidemargin=.2in
\evensidemargin=.2in
\textwidth=5.9in
\topmargin=-.5in
\textheight=10in

\definecolor{Orange}{rgb}{1,0.647,0}

\def\thesection{\Roman{section}}
\def\thesubsection{\mbox{\arabic{subsection}}}


\usepackage{geometry}
\geometry{left=0.7in,right=0.7in,top=0.7in,bottom=0.7in}

%\newcommand{\msu}{MSU}
%\newcommand{\ie}{{\sl i.e.}}
%\newcommand{\eg}{{\sl e.g.}}
%\newcommand{\etc}{{\sl etc.}}
%\newcommand{\etal}{{\sl et al.\ }}
%\newcommand{\Comment}[1]{}
%
%\newcommand{\seperator}{\vspace{0.25in}}
%
%\newcommand{\proc}{Proceedings of the }
%\newcommand{\conf}{Conference }
%\newcommand{\inte}{International }
%
\newtheorem{thm}{\textbf{Theorem}}
\newtheorem{lem}{\textbf{Lemma}}
\newtheorem{definition}{\textbf{Definition}}
%\newcommand{\wrapbox}[1]{\framebox{\begin{tabular}{c}#1\end{tabular}}}

\begin{document}
\title{Revision Plan for SIGCOMM'24 Paper \#293}
	%\titlespacing*{\section} {0pt}{9pt}{0pt}
	%\titlespacing*{\paragraph} {0pt}{8pt}{1em}
\date{}
\maketitle

\noindent Dear Prof. Praveen Kumar,\\

\noindent Thank you for shepherding our paper (\textit{\#293 PPT: A Pragmatic Transport for Datacenters}). 
We carefully went through the reviews and outlined our revision plan.\\

%\noindent In particular, we will reorganize Section 5.2.2 and add additional text in this section to clarify the reviewer's concerned points of never-seen-before-flows and coexisting application scenarios (see Response 1). Further, we will add discussions about the deployability of QCLIMB in higher-speed networks (see Response 2).\\



\noindent Please let us know if you have any feedback. Thank you for your time and help to improve the quality of our paper! \\

\noindent Best Regards,

\noindent Authors of SIGCOMM'24 Paper \#293

%\section{A Summary of Changes}
%%\noindent {\it Dear authors the paper was discussed in detail at the PC meeting and online. The reviewers are found the paper interesting but there are still some aspects of it that requires further work. We ask authors to revise and resubmit.}
%
%\textcolor{blue}{We thank the reviewers and the PC for their careful consideration of our Fall'23 paper \#160. We have revised it as the Spring'24 paper \#47. We address the meeting discussion questions in the responses below, but summarize our revision work here.}
%
%\vspace{-15pt}
%\textcolor{blue}{
%	\begin{itemize}[leftmargin=*]
%		\item[1.] As requested, we have added the implementation of RF inference module in Linux kernel to make QCLIMB work as an end-to-end system (see the first three paragraphs of Section 4.1) and accordingly conducted a clearer performance breakdown for QCLIMB as it compares to PIAS (see Figure 15). We also have conducted an experiment to breakdown loss events into different flow types to show why QCLIMB unpredictably delivers a slightly lower tail FCT than pFabric in some cases (see Table 4). 
%		\item[2.] As requested, we have completely rearchitected our testbed evaluation and re-run all experiments with higher link speeds---25Gbps (see Figures 7, 8, 9, 10, 11, 12, 13, 14, 16, 21, 22, 27 and Table 3).
%		\item[3.] We have also evaluated QCLIMB in cases with model-application mismatching, coexisting applications, varying QCLIMB-enabled flow ratio, and tiny Tensorflow workload, to address the significant methodological concerns reviewers raised (in particular, see our responses to Reviewer E's feedback), thus ensuring a fair comparison with PIAS (see Figures 17, 18, 19, 20).
%		\item[4.] We have provided two datacenter application examples to generalize our observations (see Section 2.3.1) and discussed the scenarios where these observations might not manifest (see the end of Section 2.3.2).
%		\item[5.] We have reorganized the Section 1, Section 2 and Section 3 to improve the presentation. We have also added a new Figure 3 to make QCLIMB scheduling clearer.
%		\item[6.] To make room for some of the above changes, we have moved results on ``Impact of Slack Size $S$'' to Appendix D.
%		\item[7.] We have addressed the following reviewer-raised issues in the Appendix:
%(i) Comparison with Homa (with and without perfect knowledge)$\rightarrow$Appendix D. (ii) Discuss the paper Aequitas$\rightarrow$Appendix E.
%	\end{itemize}
%}
%
%%\textcolor{blue}{As requested, we have conducted a clearer performance breakdown for QCLIMB as it compares to PIAS and pFabric, and completely rearchitected our testbed experimental evaluation and re-run all experiments with 25Gbps link speeds.
%%We have also evaluated QCLIMB in cases with model-application mismatching, coexisting applications, varying QCLIMB-enabled flow ratio, and tiny Tensorflow workload, to address the significant methodological concerns reviewers raised (in particular, see our responses to Reviewer E's feedback), thus ensuring a fair comparison with PIAS. 
%%Furthermore, we have provided two datacenter application examples to generalize our observations as well as discussed the scenarios where these observations might not manifest.
%%We have also added the implementation of RF model inference module in the Linux kernel to make our QCLIMB work as an end-to-end system.}
%
%\textcolor{blue}{We are happy to address additional reviewer comments in the final version.}
%
%\textcolor{blue}{NOTE: Our supplementary document with highlighted differences only reflects the major changes we made to address reviewer comments, specifically: new sections, paragraphs or sentences added in the main body and the appendix to address review concerns. It does not highlight the many changes we made to text and figures throughout the paper (e.g., point 5 above).}
 

%in the reviews. Below we summarize our revision based on the four~reviews.
\setlength{\parskip}{0.1em}
\section{Top-level Concerns: }
%\noindent The changes made in the revised paper according to comments of Reviewer 1 are highlighted in \textcolor{blue}{blue}.
{\it \paragraph{Concern 1:} Disconnect between the proposed goals vs the evaluation. The paper pitches itself to deliver comparable performance to proactive transports while being readily deployable. But the evaluations do not necessarily back this up.  }


\noindent\textcolor{blue}{\textbf{Response 1:} 
Thanks for the reviewer's comments. 
The proactive transports proactively allocate the bottleneck link bandwidth and prioritize it to flows with fewer remaining bytes.
Proactive transport typically displays the performance of the large and small flows, respectively. 
To support our claim of "comparable performance to proactive transports."
First, we compare the overall performance of the different transports through the overall average FCT.
Second, we show the average FCT and 99th percentile tail FCT for small flows to illustrate the ability of different transport to avoid small flow packet scheduling and queuing delays.
Finally, we show the average FCT for large flows to display the gap between different schemes' ability to utilize the spare bandwidth gracefully.
These are the performance metrics that proactive transports focus on and work with.
Therefore, we use them as the primary performance metrics for simulation and testbed and conclude they are comparable to performance in proactive transport.
To show more details, we have shown the lines of code required to be modified to deploy PPT and Homa-Linux, respectively (Tables 3 and 4), and the CPU usage of PPT and DCTCP under different loads (Figure 21) in the appendix.
We believe that the above performance metrics support our proposed goals well.
  }

{\it \paragraph{Concern 2:} There's lacking discussion on other related work such as PCC Proteus.}



\noindent\textcolor{blue}{\textbf{Response 2:} 
Thanks for the reviewer's comments. 
PCC Proteus divides flows into primary and scavenger flows according to different application requirements and proactively reduces scavenger flow bandwidth to minimize the impact on the primary flow, thus improving the overall user experience.
By contrast, PPT is insensitive to the application type.
In PPT, all flows equally utilize the bandwidth.
PPT's proposed purpose of buffer-aware flow scheduling is to ensure that small flows bypass queued packets of most large flows in the network to optimize the FCT.
Therefore, PPT and PCC Proteus are two lines of work due to different flow behaviors and optimization goals.
We will add a discussion of PCC Proteus and other related work in Appendix B.
}

{\it \paragraph{Concern 3:} The paper needs to discuss more insights on how the system is able to deliver the performance benefits.}


\noindent\textcolor{blue}{\textbf{Response 3:} 
Thanks for the reviewer’s comments. 
In the overall average performance analysis in Section 6.2, we mentioned that the PPT benefits from gracefully utilizing the spare bandwidth without causing bandwidth waste or sending opportunistic packets too aggressively.
To further explain the insights, we add a remark at the end of Appendix C.1 to analyze how the PPT achieves its superior performance in more detail.
}

{\it \paragraph{Concern 4:} Other claims such as "integration with other transports is easy" need to be further justified.}


\noindent\textcolor{blue}{\textbf{Response 4:} 
Thanks for the reviewer's comments. 
We implemented a delay-based transport conceptually equivalent to Swift in the ns3 simulator.
It doesn't include the component in Swift that handles endpoint congestion because (1) PPT doesn't focus on endpoint congestion, and (2) the ns3 simulator hardly simulates endpoint congestion as well as it does. 
We compared this variant with the original delay-based transport and plotted the result in Figure 26.
We find that when incorporating PPT's design with the original delay-based transport, the overall average FCT, the average//tail FCT of small flows, and the average FCT of large flows can be reduced by 16.7\%, 56.5\%/72.1\%, and 11\%, respectively.
To show more details, we plan to add a more detailed description of this variant in the corresponding section of Appendix C.3.
We also plan to add a remark paragraph at the end of the chapter to discuss how delay-based transport can benefit from the design of the PPT in further depth.
}

{\it \paragraph{Concern 5:}Missing details on evaluation setup and parameters.}


\noindent\textcolor{blue}{\textbf{Response 5:}
Thanks for the reviewer’s comments. 
We record the maximum value of the window from the beginning of the flow until now as MW (Maximum Window).
Section 2.3 first shows how much DCTCP can benefit from padding the spare bandwidth to MW and plots the results in Figure 2.
Moreover, we show that padding the spare bandwidth to MW is just suitable without wasting bandwidth or causing congestion throughout the experiment, and we plot the results of our experiment in Figure 3.
Another question is about specific TCP send buffer thresholds.
To reproduce the behavior of the actual deployed application as much as possible, we selected specific application traces for the Memcached application and the web server application, respectively.
The flow size distribution in specific application traces is concentrated in particular intervals, e.g., there is no flow in the ETC trace of the paper[8] that exceeds 100KB.
Under such traces, we set different thresholds depending on the traces to verify the effectiveness of the buffer-aware flow identification.
In the large-scale simulation and testbed, the TCP send buffer threshold is set to the fixed value of 100KB, with outstanding results.
To evaluate if this can benefit PPT’s performance, we construct a PPT variant that turns off this approach and considers all flows non-identified.
We plot the results in Figure 20, showing that this improves the small flow performance as expected.
In summary, the TCP send buffer threshold does not assume that the application type and flow size distribution are known a priori and effectively improves small flow performance.
}
	
\section{Other Concerns}
%\noindent The changes made in the revised paper according to comments of Reviewer 1 are highlighted in \textcolor{blue}{blue}.
\subsection{Reviewer \#293A}
{\it \paragraph{A.1: Reviewer Comment:} The number of used queues could be clarified: p. 6 mentions two such queues (a high-priority and a low-priority queue with a different $\lambda$. But then it mentions that when queueing the opportunistic (presumably low-priority) packets, this may hurt normal (presumably high-priority) packets, suggesting they share the queue. Then, p. 8 mentions 8 priorities, such that "switches can use strict priority to dequeue packets", implicitly pointing to 8 different queues. Then, p.9 mentions 2 queues again. }


\noindent\textcolor{blue}{\textbf{Response A.1:}
Thanks for the reviewer’s comments. 
PPT uses eight strict priorities in the switch.
To prevent HCP traffic harmed by LCP, priorities 0$\sim$3 are used to transmit HCP packets, and priorities 4$\sim$7 are used to transmit LCP packets.
Therefore, the \emph{high-priority} and \emph{low-priority} mentioned in p.6 and p.9 refer to priorities 0$\sim$3 and 4$\sim$7 in the switch, respectively.
To avoid ambiguity, we will include footnotes in Section 3.2 for explanation.
}


{\it \paragraph{A.2: Reviewer Comment:} In addition, PPT seems to couple each LCP flow with an HCP flow, by defining the window size of the LCP flow using the window size of the associated HCP flow. But what if there are no HCP flows now? Are LCP flows stuck? Or more generally, what if the numbers of flows are not equal? Or what if they are equal but the flows are destined to different destinations? How does this coupling help? }	

\noindent\textcolor{blue}{\textbf{Response A.2:}
Thanks for the reviewer's comments. 
Although very sorry, the reviewer seems to have misinterpreted PPT's strategy.
At the beginning of Section 3, we mentioned that "LCP sends opportunistic packets from the tail end."
Therefore, both HCP and LCP send data in the same flow.
However, many of the review's comments were based on LCP and HCP transmitting data from different flows, which made it difficult for us to answer.
}

\subsection{Reviewer \#293B}
{\it \paragraph{B.1: Reviewer Comment:} During LCP loop initialization for case 1, it is unclear how PPT computes BDP in the first RTT. As you mention, DCTCP is still probing for available bandwidth.}

\noindent\textcolor{blue}{\textbf{Response B.1:}
Thanks for the reviewer’s comments. 
Since the NIC rate can be read via system call and the datacenter network topology is relatively fixed, we assume the RTT is known a priori.
By NIC rate times RTT, we assume the host knows the BDP in advance.
}

{\it \paragraph{B.2: Reviewer Comment:} I wonder if tracking $\alpha_{min}$ can lead to underutilization due to network dynamics. Suppose we have a long flow which experiences transient congestion at the beginning (say due to an incast); the trans ient congestion causes $\alpha_{min}$ to be $>$ 0.5 for the flow. After the transient congestion subsides, the large flow is unable to utilize any spare bandwidth arising from Case 2.}

\noindent\textcolor{blue}{\textbf{Response B.2:}
Thanks for the reviewer’s comments. 
During the process of congestion subsiding, the switch queue length is reduced, the percentage of ECN marked packets for this long flow decreases, and the value of $\alpha$.
As the congestion subsides completely, the proportion of ECN-marked packets should be 0.
PPT initializes LCP for the long flow whenever $\alpha$ takes the minimum value.
At this point, $\alpha$ must be less than 0.5, which makes this large flow utilize the spare bandwidth.
}

{\it \paragraph{B.3: Reviewer Comment:} It is unclear how PPT deals with fairness as flows arrive and leave. Suppose two large flows sharing a common bottleneck link arrive one after the other. The latter flow might see much lower $W_{max}$ (50\%?) than the earlier flow. In every LCP loop initialization (case 2), the latter flow would compute a lower value of initial congestion window and send less packets via LCP compared to the earlier flow.}


\noindent\textcolor{blue}{\textbf{Response B.3:}
Thanks for the reviewer’s comments. 
}

{\it \paragraph{B.4: Reviewer Comment:} As the paper targets the issue of underutilizing high datacenter bandwidth, I wonder if some of the simulations could have been done at higher line rates (400+ Gbps) to demonstrate the problem more clearly.}


\noindent\textcolor{blue}{\textbf{Response B.4:}
Thanks for the reviewer’s comments. 
We will add a new simulation experiment with topology consisting of 144 servers, 9 leaf switches, and 4 spine switches, with the host and core links operated at 400 and 1000Gbps, respectively.
We will plot the results in the appendix.
}

{\it \paragraph{B.5: Reviewer Comment:} What are the overheads of PPT?}

\noindent\textcolor{blue}{\textbf{Response B.5:}
Thanks for the reviewer’s comments. 
Actually, we measure the kernel space CPU overhead of PPT and DCTCP in our testbed.
We plot the results in Figure 21.
For more details, please read Appendix C.1.
}

\subsection{Reviewer \#293C}
{\it \paragraph{C.1: Reviewer Comment:} It would be particularly useful to have an intuitive and straightforward example scenario where LCP can discover spare bandwidth other than during the slow start phase.}

\noindent\textcolor{blue}{\textbf{Response C.1:}
Thanks for the reviewer’s comments. 
Indeed, in section 2.3, we mention that \emph{DCTCP marks ECN at the switch for arriving packets if queue occupancy exceeds a threshold K, and the sender cuts the window based on the fraction of ECN marked ACKs. So, when there are multiple concurrent flows, DCTCP may mark ECN for packets from many flows, which may cut windows simultaneously, thus causing a sudden drain on the switch buffer and leaving bandwidth underutilized.}
To show this point, We run ns-3 simulations with two senders and one receiver sharing the bottleneck. 
We sample the bottleneck link utilization every 100us for 10ms when DCTCP enters a steady state.
We plot the results in Figure 1, showing that nearly half of the bandwidth is underutilized.
}

{\it \paragraph{C.2: Reviewer Comment:} Figure 13 doesn’t add up by itself. RC3 has the highest average FCT for small and large flows. However, its overall average is not the highest. Given that large and small flows account for 13\% and 87\% respectively, the overall average should be around 0.13 * 39.63 + 0.87 * 0.77 = 5.8? Or am I missing something?}


\noindent\textcolor{blue}{\textbf{Response C.2:}
Thanks for the reviewer’s comments. 
After carefully checking our data processing code, we found that under Datamining workloads, RC3 would result in a few flows not completing due to many packet losses.
Our code incorrectly used the program runtime as the flow completion time when calculating the overall average FCT, resulting in the actual computed result being larger.
We will fix the bug and update the correct result.
}

{\it \paragraph{C.3: Reviewer Comment:} Page 12, can you give an intuition why limiting RC3’s low priority queue doesn’t help? It kind of contradicts the argument that aggressive low priority packets hinder high-priority packets being a problem if the high-priority queue has enough buffers?}


\noindent\textcolor{blue}{\textbf{Response C.3:}
Thanks for the reviewer's comments. 
Although limiting the RC3 low-priority queue can prevent the LCP from sending too many opportunity packets, the following reasons still lead to low performance.
First, during the slow-start phase of the flow, limiting the buffer LCP could use will leave a large amount of bandwidth underutilized.
Second, RC3 keeps the LCP open instead of intermittent initialization as PPT, which causes more severe congestion during traffic bursts.
Last but not least, RC3's lack of using in-network priority causes small flow packets to be queued after LCP packets, increasing small flow latency.
We will add the above analyses to our paper.
}


\subsection{Reviewer \#293D}
{\it \paragraph{D.1: Reviewer Comment:} I'm not sure if I fully understand some of the experimental results. To give an example, why is Homa's performance so poor for the testbed incast experiment? You mention that PPT is effective because it can recover quickly but why can't Homa?}


\noindent\textcolor{blue}{\textbf{Response D.1:}
Thanks for the reviewer’s comments. 
As mentioned in Section 6.1.2, Homa-Linux sends BDP-sized unscheduled packets for each arriving flow, and excessive opportunity packets cause heavier congestion in incast scenarios.
Furthermore, Homa-Linux is connectionless, so packet loss recovery is achieved through retransmission after timeout.
Large packet loss in Incast scenarios further impairs its performance.
We will add more detailed analyses in the paper.
}

\subsection{Reviewer \#293E}

{\it \paragraph{E.1: Reviewer Comment:} In production systems, priority queues are scarce resource. Using only two HW queues for one application will be a luxury. How does PTP perform with 2 prio queues (one high, one low) compared to existing production solutions like Swift, HPCC, DCTCP?}

\noindent\textcolor{blue}{\textbf{Response E.1:}
Thanks for the reviewer’s comments. 
In our paper, we construct a stricter variant of PPT that assigns all packets the same priority.
We plot the results in Figure 19.
Even under these conditions, we find that PPT still achieves the optimal overall average FCT among NDP, Homa, Aeolus, DCTCP, and RC3.
}

{\it \paragraph{E.2: Reviewer Comment:} Even the 'large-scale' simulations cover only 144 servers w/ 40~100G as core link speeds. This resembles DC topology of more a decade ago. With the trend of AI clusters growing to have 10s of Ks of GPUs, I believe at least 2K or more endpoints must be simulated to see the real impact on shallow switch.}

\noindent\textcolor{blue}{\textbf{Response E.2}
Thanks for the reviewer’s comments. 
We will add a large-scale simulation experiment with 2k nodes to the appendix.
} 

\begin{spacing}{-1.0}
\bibliographystyle{IEEEtran}
\vspace{-0.1in}
\bibliography{bibfile}
\end{spacing}
\end{document}

##########################################################################################################
